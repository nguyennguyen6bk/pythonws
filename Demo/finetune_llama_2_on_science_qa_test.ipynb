{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5275,"status":"ok","timestamp":1692629330209,"user":{"displayName":"Nguyen Nguyen","userId":"06081248861065786860"},"user_tz":-420},"id":"Pbp2GypOhOlQ","outputId":"5c249d91-b933-4575-8bce-67a08a63b0b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: scale-llm-engine in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.0b8)\n","Requirement already satisfied: aiohttp<4.0,>=3.8 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scale-llm-engine) (3.8.4)\n","Requirement already satisfied: pydantic<2.0,>=1.10 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scale-llm-engine) (1.10.12)\n","Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scale-llm-engine) (2.31.0)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0,>=3.8->scale-llm-engine) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0,>=3.8->scale-llm-engine) (3.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0,>=3.8->scale-llm-engine) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0,>=3.8->scale-llm-engine) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0,>=3.8->scale-llm-engine) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0,>=3.8->scale-llm-engine) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0,>=3.8->scale-llm-engine) (1.3.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<2.0,>=1.10->scale-llm-engine) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.31.0->scale-llm-engine) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.31.0->scale-llm-engine) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nuc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.31.0->scale-llm-engine) (2022.12.7)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install scale-llm-engine"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1692629330209,"user":{"displayName":"Nguyen Nguyen","userId":"06081248861065786860"},"user_tz":-420},"id":"FMFEE_5XjmsQ","outputId":"fa24e208-6f14-47c8-fc67-36718277480f"},"outputs":[{"name":"stdout","output_type":"stream","text":["cllkzz7jz06fk1a0v3yht1v11\n"]}],"source":["from llmengine import Completion\n","import os\n","\n","# Replace '[Your API key]' with your actual API key\n","os.environ['SCALE_API_KEY'] = 'cllkzz7jz06fk1a0v3yht1v11'\n","api_key = os.environ.get('SCALE_API_KEY')\n","print(api_key)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1692629330712,"user":{"displayName":"Nguyen Nguyen","userId":"06081248861065786860"},"user_tz":-420},"id":"N814xL0unDt8","outputId":"93bda163-ee09-4d9d-87e7-042ee71e3001"},"outputs":[{"ename":"NotFoundError","evalue":"Not Found","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[39m=\u001b[39m Completion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllama-2-7b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      3\u001b[0m     prompt\u001b[39m=\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\u001b[39mContext: Jeremiah has a pet lizard. Jeremiah notices that on some days, the lizard is active and runs around the tank. On other days, the lizard hardly moves at all. Jeremiah wonders what factors affect how active his lizard is. So, he decides to design an experiment. He has the following supplies available:\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[39m    one pet lizard\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[39m    live crickets\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[39m    live mealworms\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[39m    one heating lamp\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[39m    Question: Using only these supplies, which question can Jeremiah investigate with an experiment?\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[39m    Options:(A) Is the pet lizard more active when its tank is heated with one heating lamp or with two heating lamps? (B) Is the pet lizard more active when it is fed insects or lettuce? (C) Is the pet lizard more active when it is fed crickets or mealworms?\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[39m    Answer:\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[0;32m     12\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mtext)\n","File \u001b[1;32mc:\\Users\\NUC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\llmengine\\completion.py:294\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, model, prompt, max_new_tokens, temperature, timeout, stream)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     data \u001b[39m=\u001b[39m CompletionSyncV1Request(\n\u001b[0;32m    292\u001b[0m         prompt\u001b[39m=\u001b[39mprompt, max_new_tokens\u001b[39m=\u001b[39mmax_new_tokens, temperature\u001b[39m=\u001b[39mtemperature\n\u001b[0;32m    293\u001b[0m     )\u001b[39m.\u001b[39mdict()\n\u001b[1;32m--> 294\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mpost_sync(\n\u001b[0;32m    295\u001b[0m         resource_name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mv1/llm/completions-sync?model_endpoint_name=\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    296\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    297\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    298\u001b[0m     )\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m CompletionSyncResponse\u001b[39m.\u001b[39mparse_obj(response)\n","File \u001b[1;32mc:\\Users\\NUC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\llmengine\\api_engine.py:106\u001b[0m, in \u001b[0;36mAPIEngine.post_sync\u001b[1;34m(cls, resource_name, data, timeout)\u001b[0m\n\u001b[0;32m     98\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[0;32m     99\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(LLM_ENGINE_BASE_PATH, resource_name),\n\u001b[0;32m    100\u001b[0m     json\u001b[39m=\u001b[39mdata,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m     auth\u001b[39m=\u001b[39m(api_key, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    104\u001b[0m )\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m parse_error(response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mcontent)\n\u001b[0;32m    107\u001b[0m payload \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[0;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m payload\n","\u001b[1;31mNotFoundError\u001b[0m: Not Found"]}],"source":["response = Completion.create(\n","    model=\"llama-2-7b\",\n","    prompt=\"\"\"Context: Jeremiah has a pet lizard. Jeremiah notices that on some days, the lizard is active and runs around the tank. On other days, the lizard hardly moves at all. Jeremiah wonders what factors affect how active his lizard is. So, he decides to design an experiment. He has the following supplies available:\n","    one pet lizard\n","    live crickets\n","    live mealworms\n","    one heating lamp\n","    Question: Using only these supplies, which question can Jeremiah investigate with an experiment?\n","    Options:(A) Is the pet lizard more active when its tank is heated with one heating lamp or with two heating lamps? (B) Is the pet lizard more active when it is fed insects or lettuce? (C) Is the pet lizard more active when it is fed crickets or mealworms?\n","    Answer:\"\"\",\n","    max_new_tokens=100,\n","    temperature=0.2,\n",")\n","\n","print(response.output.text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":521,"status":"ok","timestamp":1692629331231,"user":{"displayName":"Nguyen Nguyen","userId":"06081248861065786860"},"user_tz":-420},"id":"O1uT-RNwdKnu","outputId":"13103aa4-7d21-404b-ec9c-d30c811d1ca5"},"outputs":[],"source":["import sys\n","stream = Completion.create(\n","    model=\"llama-2-7b\",\n","    prompt=\"\"\"Context: Jeremiah has a pet lizard. Jeremiah notices that on some days, the lizard is active and runs around the tank. On other days, the lizard hardly moves at all. Jeremiah wonders what factors affect how active his lizard is. So, he decides to design an experiment. He has the following supplies available:\n","    one pet lizard\n","    live crickets\n","    live mealworms\n","    one heating lamp\n","    Question: Using only these supplies, which question can Jeremiah investigate with an experiment?\n","    Options:(A) Is the pet lizard more active when its tank is heated with one heating lamp or with two heating lamps? (B) Is the pet lizard more active when it is fed insects or lettuce? (C) Is the pet lizard more active when it is fed crickets or mealworms?\n","    Answer:\"\"\",\n","    max_new_tokens=3,\n","    temperature=0.2,\n","    stream=True,\n",")\n","\n","for response in stream:\n","    if response.output:\n","        print(response.output.text, end=\"\")\n","        sys.stdout.flush()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4846,"status":"ok","timestamp":1692629336074,"user":{"displayName":"Nguyen Nguyen","userId":"06081248861065786860"},"user_tz":-420},"id":"6HJNOxPAmPkD","outputId":"4933b77d-8f08-4ecc-c807-c6f0897b1414"},"outputs":[],"source":["\n","\n","response = Completion.create(\n","    model=\"falcon-7b-instruct\",\n","    prompt=\"I'm opening a pancake restaurant that specializes in unique pancake shapes, colors, and flavors. List 3 quirky names I could name my restaurant.\",\n","    max_new_tokens=100,\n","    temperature=0.2,\n",")\n","\n","print(response.output.text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2157,"status":"ok","timestamp":1692629338228,"user":{"displayName":"Nguyen Nguyen","userId":"06081248861065786860"},"user_tz":-420},"id":"xC-hhajpZKOf","outputId":"0296bba8-0511-4244-facd-5f6ce9d94e56"},"outputs":[],"source":["import sys\n","stream = Completion.create(\n","    model=\"llama-2-7b\",\n","    prompt=\"\"\"Context: Trade happens when people agree to exchange goods and services. People give up something to get something else. Sometimes people barter, or directly exchange one good or service for another.\n","Maureen and Kendrick open their lunch boxes in the school cafeteria. Neither Maureen nor Kendrick got everything that they wanted. The table below shows which items they each wanted:\n","Look at the images of their lunches. Then answer the question below.\n","Maureen's lunch Kendrick's lunch\n","Question: What can Maureen and Kendrick trade to each get what they want?\n","Options:(A) Kendrick can trade his broccoli for Maureen's oranges. (B) Kendrick can trade his almonds for Maureen's tomatoes. (C) Maureen can trade her tomatoes for Kendrick's broccoli. (D) Maureen can trade her tomatoes for Kendrick's carrots.\n","Answer:\"\"\",\n","    max_new_tokens=100,\n","    temperature=0.2,\n","    stream=True,\n",")\n","\n","for response in stream:\n","    if response.output:\n","        print(response.output.text, end=\"\")\n","        sys.stdout.flush()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2100,"status":"ok","timestamp":1692629340326,"user":{"displayName":"Nguyen Nguyen","userId":"06081248861065786860"},"user_tz":-420},"id":"Q8dirjdZXJik","outputId":"15fbff9b-cfde-464a-9fef-6217f5ab6e8f"},"outputs":[],"source":["response = Completion.create(\n","    model=\"falcon-7b-instruct\",\n","    prompt=\"\"\"Context: The passage below describes an experiment. Read the passage and then follow the instructions below.\n","Kathleen applied a thin layer of wax to the underside of her snowboard and rode the board straight down a hill. Then, she removed the wax and rode the snowboard straight down the hill again. She repeated the rides four more times, alternating whether she rode with a thin layer of wax on the board or not. Her friend Bryant timed each ride. Kathleen and Bryant calculated the average time it took to slide straight down the hill on the snowboard with wax compared to the average time on the snowboard without wax.\n","Figure: snowboarding down a hill.\n","Question: Identify the question that Kathleen and Bryant's experiment can best answer.\n","Options:(A) Does Kathleen's snowboard slide down a hill in less time when it has a layer of wax or when it does not have a layer of wax? (B) Does Kathleen's snowboard slide down a hill in less time when it has a thin layer of wax or a thick layer of wax?\n","Answer:\"\"\",\n","    max_new_tokens=100,\n","    temperature=0.2,\n",")\n","\n","print(response.output.text)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1ITvzDbWiBKbVhNfkevriSWy4ryB8P3kF","timestamp":1692608754048}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}
