{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dswh/lil_nlp_with_tensorflow/blob/main/02_05_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTguFckTEDWd"
      },
      "source": [
        "# Text Classification challenge\n",
        "\n",
        "You are required to train a deep learning model on the IMDB reviews dataset and classify a set of new reviews as positive(1) or negative(0) using the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mW3Mt2q5kL2",
        "outputId": "3f76c460-d906-4022-ff8e-3e91996ee4fd"
      },
      "outputs": [],
      "source": [
        "##import the required libraries and APIs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rhw0j_s5UZ2"
      },
      "source": [
        "## Downloading the TensorFlow `imdb_review` dataset\n",
        "\n",
        "> Make sure tensorflow_datasets is installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx_DJfb7EFHh"
      },
      "outputs": [],
      "source": [
        "##load the imdb reviews dataset\n",
        "data, info = tfds.load(\"____\", with_info=____, as_supervised=____)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MBqFTBP6DT4"
      },
      "source": [
        "## Segregating training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM2X1wLvUb8n"
      },
      "outputs": [],
      "source": [
        "##segregate training and test set\n",
        "train_data, test_data = data['____'], data['____']\n",
        "\n",
        "##create empty list to store sentences and labels\n",
        "train_sentences = []\n",
        "test_sentences = []\n",
        "\n",
        "train_labels = []\n",
        "test_labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxoAZl0gU_y-"
      },
      "outputs": [],
      "source": [
        "##iterate over the train data to extract sentences and labels\n",
        "for sent, label in ____:\n",
        "    train_sentences.append(str(sent.numpy().decode('utf8')))\n",
        "    train_labels.append(label.numpy())\n",
        "\n",
        "##iterate over the test set to extract sentences and labels\n",
        "for sent, label in ____:\n",
        "    test_sentences.append(str(sent.numpy().decode('utf8')))\n",
        "    test_labels.append(label.numpy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDKl0NzBITfe"
      },
      "outputs": [],
      "source": [
        "##convert lists into numpy array\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLIjftvF6IRZ"
      },
      "source": [
        "## Data preparation - setting up the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mqx-tgBVXQz"
      },
      "outputs": [],
      "source": [
        "##define the parameters for the tokenizing and padding\n",
        "vocab_size = ____\n",
        "embedding_dim = ____\n",
        "max_length = ____\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYsZatAaVmfq"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words = ____, oov_token=____)\n",
        "tokenizer.fit_on_texts(____)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "##training sequences and labels\n",
        "train_seqs = tokenizer.texts_to_sequences(____)\n",
        "train_padded = pad_sequences(____, maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "##testing sequences and labels\n",
        "test_seqs = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_seqs,maxlen=max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcvfYesOIo3A"
      },
      "source": [
        "## Define the Neural Network with Embedding layer\n",
        "\n",
        "1. Use the Sequential API.\n",
        "2. Add an embedding input layer of input size equal to vocabulary size.\n",
        "3. Add a flatten layer, and two dense layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF6ict6vWJAV",
        "outputId": "0408dce9-170c-45e2-e797-46e0737ea162"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.____(____, ____, input_length=____),\n",
        "    tf.keras.layers.____(),\n",
        "    tf.keras.layers.____(24, activation='relu'),\n",
        "    tf.keras.layers.____(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "##compile the model with loss function, optimizer and metrics\n",
        "model.compile(loss='____',optimizer='____',metrics=['accuracy'])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlNRXgJ99Dv9"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S9zFDyLWZDF",
        "outputId": "415986ef-cfd7-48fc-8b4d-09009628645b"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "##train the model with training and validation set\n",
        "history = model.fit(\n",
        "    ____,   #training sequence\n",
        "    ____, # training labels\n",
        "    epochs=num_epochs, \n",
        "    validation_data=(____, ____) # test data\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCHKeE8Z9Gm9"
      },
      "source": [
        "## Visualise the train & validation accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "_U2cCtIY9bA3",
        "outputId": "07810114-f05d-49ca-9e1d-8add55c308eb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "##plot the scores from history\n",
        "def plot_metrics(history, metric):\n",
        "  plt.plot(history.history[____])\n",
        "  plt.plot(history.history['val_'+ ____])\n",
        "  plt.legend([string, 'val_'+____])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(____)\n",
        "  plt.show()\n",
        "  \n",
        "##plot accuracy\n",
        "plot_metrics(history, \"____\")\n",
        "\n",
        "##plot loss\n",
        "plot_metrics(history, \"____\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy46FyKEcqxq"
      },
      "source": [
        "## Classify new reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCHIxcxBcuBg",
        "outputId": "e85d0386-349e-42fd-9439-c21493e19994"
      },
      "outputs": [],
      "source": [
        "sentence = [\"The first part of the movie was dull and boring!\", \"We watched Queen's Gambit, all seven hours of it, in a single sitting. This show is an absolute gem.\"]\n",
        "\n",
        "##prepare the sequences of the sentences in question\n",
        "sequences = tokenizer.____(____)\n",
        "padded_seqs = ____(____, maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "##print the classification score\n",
        "print(model.predict(____))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOIqkYammdNS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMrvRsvVhsLSa0zSCObtgI4",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "02_05_challenge.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
